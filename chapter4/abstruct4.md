# ニューラルネットワークの学習
訓練データから最適な重みパラメータの値を自動で獲得することを指す

ニューラルネットワークの特徴はデータから学習を行い、重みをそのデータから自動的に決定することができることである

本章で行うことは、ある問題を正しく分類するためのプログラムを自分で設計することではなく、データを有効活用して正しく分類できるようにすることである。その一つの方法として**特徴量**がある。例えば、画像から特徴量を抽出してそのパターンを機械学習の技術で学習することができる。

## 訓練データとテストデータ
機械学習の問題では訓練データとテストデータの2つのデータに分けて学習や実験を行うことが一般的である。

※訓練データは**教師データ**と呼ばれることもある

訓練データとテストデータを分ける理由としては、求めているものが汎化能力（訓練データに含まれないデータ）を正しく評価したいからである。

## 損失関数
ニューラルネットワークの学習には指標として**損失関数**を利用する。損失関数は任意の関数を用いることができるが、一般的には2乗和誤差や交差エントロピー誤差などが用いられます

### 2乗和誤差
損失関数として用いられる最も有名な関数
```
E = (1/2) Σ (yk - tk)^2
```
yk…ニューラルネットワークの出力
tk…教師データ
k…次元数（Σの下につけたかった）

### 交差エントロピー誤差
損失関数として用いられる
```
E = - Σ tk log yk
```
yk…ニューラルネットワークの出力
tk…教師データ
k…次元数（Σの下につけたかった）

### ミニバッチ学習
MNISTのデータは60,000個あるが、すべてのデータに関して損失関数の和を求めるには時間がかかる。
そこで、60.000枚の中から無作為に100枚を選び出し、ミニバッチ（小さな塊）ごとに学習を行う。
このような学習方法を**ミニバッチ学習**と言う

### なぜ損失関数を設定するのか
ニューラルネットワークの学習では最適なパラメータ(重みとバイアス)を探索するときに、損失関数ができるだけ小さな場所を探すようにするために微分を計算して、パラメータを更新していく。
例えば、ニューラルネットワークのある一つの重みに注目し、それの損失関数に対する微分は「重みパラメータの値を少しだけ変化させたときに、損失関数がどのように変化するか」を表している。
もし、微分した値がマイナスになるならば、その重みを正の方向へ変化させることで損失関数を減少させることができる。逆もまたしかりである。

例として、「数字認識」を行うときは認識精度を指標にすればよいではないか！と思えるが、認識精度の微分はほとんどの場所で0になってしまい、その時点でパラメータの更新ができるなくなる。
よって損失関数を設定するのである。

