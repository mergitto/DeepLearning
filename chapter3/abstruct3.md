# ニューラルネットワーク
ニューラルネットワークはパーセプトロンのデメリットを解決できる

### パーセプトロンの得意分野
- 複雑な処理を表現できる
- 非線形な表現が可能になる

### パーセプトロンの苦手分野
- 重みの設定は手作業

ニューラルネットワークは適切な重みを自動で学習できるという重要な性質を持つ
本章では概要について説明する
図の左側の列を「入力層」、図の中間の列を「中間層（隠れ層）」、図の右側の列を「出力層」という

![ニューラルネットワーク](neural.png "ニューラルネットワーク")

前章にて出てきたパーセプトロンを表す式をよりシンプルに書き換えると以下のようになる
```
y = h(b + w1x1 + w2x2)

h(x) = 0 (x <= 0)
h(x) = 1 (x > 0)
```
h(x)という関数を用いて表している

h(x)が0を超えたら出力は１、そうでなければ出力は０となる

h(x)…入力信号の総和を出力信号に変換する関数は**活性化関数**と呼ばれている

## 活性化関数
活性化関数は閾値を境にして出力が切り替わる関数である。
これは「ステップ関数」や「階段関数」と呼ばれる
つまりパーセプトロンは活性化関数にステップ関数を利用している。パーセプトロンに利用しているステップ関数を変更することでニューラルネットワークへの世界へ進むことが出来る

### シグモイド関数
シグモイド関数はニューラルネットワークでよく利用される活性化関数の内の1つである。
以下のような式で表される
```math
h(x) = 1 / 1 + exp(-x)
```

###非線形関数
sigmoid.pyやstep.pyは共通してともに**非線形関数**である（シグモイド関数とステップ関数）
ニューラルネットワークでは活性化関数にこの非線形関数を用いる必要がある<==>つまり活性化関数に線形関数を用いてはならないということ

###ReLU関数(Rectified Linear Unit)
活性化関数には古くからシグモイド関数が用いられてきたが、最近ではReLUという関数が用いられる

```
h(x) = x (x > 0)
h(x) = 0 ( x <= 0)
```
入力が0より大きいときそのまま出力し、0以下の場合0を出力する

##出力層の設計
ニューラルネットワークは分類問題と回帰問題両方に用いることができる
ただし、出力層に利用する活性化関数が変わる
- 分類問題・・・ソフトマックス関数 
- 回帰問題・・・恒等関数 
※分類問題・・・<例>人の映った画像からその人が男性か女性かを分類する 
※回帰問題・・・<例>ある入力データから数値の予測を行う 
ソフトマックス関数は、活性化関数を通して出力される値が、使用する活性化関数によっては負の値が出てきたりと、そのままでは扱いづらいため、この出力を確率に変換する式となる。

#まとめ
- ニューラルネットワークはシグモイド関数、ReLU関数
- パーセプトロンはステップ関数
