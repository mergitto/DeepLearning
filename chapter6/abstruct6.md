#学習に関するテクニック
本章ではニューラルネットワークの学習においてキーとなるものを説明します 
取り上げるテーマは
- 最適な重みパラメータを探索する最適化手法
- 重みパラメータの初期値
- ハイパーパラメータの設定方法
どれもニューラルネットワークの学習において重要なテーマである 

また過学習の対応策として
- Weight decay
- Dropout
などの正則化手法の概要説明と実装を行う

最後に
- Batch Normalization
という手法の説明をおこなう

### パラメータの更新
損失関数の値をできるだけ小さくするパラメータを見つけることを**最適化**という 
これまで勾配を使って徐々に最適なパラメータへと近づけて行ったが、それのことを**確率的勾配降下法**と呼び、**SGD**とよぶ 

### SGDの弱点
SGDは単純で実装も簡単だが、問題によっては勾配が本来の最小値ではない方向をさして非効率な場合があるのでそれに代わる手法として以下の3つの手法を説明する
- Momentum
実装したものはcommon/optimizer.pyにある 
物理でいう速度にあたる変数が登場し、何も力を受けない時には徐々に減速するような役割を担う 

- AdaGrad
ニューラルネットワークの学習では学習係数が小さすぎると時間がかかり、大きすぎると発散してしまい正しい学習を行うことができない 
この学習係数に関する有効なテクニックとして、学習係数を学習が進むにつれて小さくしていくという方法がある 
パラメータの要素ごとに適応的に更新ステップを調整するのである 

- Adam
MomentumとAdaGradを融合したような手法 
ハイパーパラメータの「バイアス補正」が行われるのも特徴 

同じディレクトリにある
- optimizer_conpare_naive.py
- optimizer_conpare_mnist.py
~naive.pyは
```
f(x,y) = 1 / 20 x^2 + y^2
```
の数式を各手法によるパラメータ更新を計算している 

~mnist.pyはMNISTデータセット(手書き数字)を対象にパラメータの更新を行なっている 


とはいえ、多くの研究ではSGDを含めそれぞれに特徴と得意・不得意があるため実験によって変更するのが良さそう！ 
一般的にはSGDよりも他の3つの手法の方が早く学習でき、認識性能も高くなることが多い 









